{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-03-17 13:24:57--  https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment_network/labels.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 225000 (220K) [text/plain]\n",
      "Saving to: ‘labels.txt.1’\n",
      "\n",
      "labels.txt.1        100%[===================>] 219.73K   252KB/s    in 0.9s    \n",
      "\n",
      "2017-03-17 13:24:59 (252 KB/s) - ‘labels.txt.1’ saved [225000/225000]\n",
      "\n",
      "--2017-03-17 13:24:59--  https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment_network/reviews.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33678267 (32M) [text/plain]\n",
      "Saving to: ‘reviews.txt.1’\n",
      "\n",
      "reviews.txt.1       100%[===================>]  32.12M   152KB/s    in 2m 59s  \n",
      "\n",
      "2017-03-17 13:28:02 (184 KB/s) - ‘reviews.txt.1’ saved [33678267/33678267]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment_network/labels.txt\n",
    "! wget https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment_network/reviews.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
    }
   },
   "source": [
    "# Curate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    if(labels[i] == 'POSITIVE'):\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            positive_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else:\n",
    "        for word in reviews[i].split(\" \"):\n",
    "            negative_counts[word] += 1\n",
    "            total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "        pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    if(ratio > 1):\n",
    "        pos_neg_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        pos_neg_ratios[word] = -np.log((1 / (ratio+0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating the Input/Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74074\n"
     ]
    }
   ],
   "source": [
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_0 = np.zeros((1,vocab_size))\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_for_label(label):\n",
    "    if(label == 'POSITIVE'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_input_layer(review):\n",
    "    \n",
    "    global layer_0\n",
    "    \n",
    "    # clear out previous state, reset the layer to be all 0s\n",
    "    layer_0 *= 0\n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1\n",
    "\n",
    "update_input_layer(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in reviews[0].split(\" \"):\n",
    "    review_counter[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encryption Logic From Previous Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keySwitch(M,c,l):\n",
    "    c_star = getBitVector(c,l)\n",
    "    return M.dot(c_star)\n",
    "\n",
    "def getRandomMatrix(row,col,bound):\n",
    "    A = np.zeros((row,col))\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            A[i][j] = np.random.randint(bound)\n",
    "    return A\n",
    "\n",
    "def getBitMatrix(S,l):\n",
    "    S_star = list()\n",
    "    for i in range(l):\n",
    "        S_star.append(S*2**(l-i-1))\n",
    "    S_star = np.array(S_star).transpose(1,2,0).reshape(len(S),len(S[0])*l)\n",
    "    return S_star\n",
    "\n",
    "def getSecretKey(T):\n",
    "    assert(T.ndim == 2)\n",
    "    I = np.eye(len(T)) # num rows\n",
    "    return hCat(I,T)\n",
    "\n",
    "def hCat(A,B):\n",
    "    return np.concatenate((A,B),1)\n",
    "\n",
    "def vCat(A,B):\n",
    "    return np.concatenate((A,B),0)\n",
    "\n",
    "def keySwitchMatrix(S, T,l):\n",
    "    S_star = getBitMatrix(S,l)\n",
    "    A = getRandomMatrix(T.shape[1],S_star.shape[1], aBound)\n",
    "    E = getRandomMatrix(S_star.shape[0], S_star.shape[1], eBound)\n",
    "    return vCat(S_star + E - T.dot(A), A)\n",
    "\n",
    "def encrypt(T, x,w,l):\n",
    "    return keySwitch(keySwitchMatrix(np.eye(len(x)),T,l), w * x,l)\n",
    "\n",
    "def addVectors(c1, c2):\n",
    "    return c1 + c2\n",
    "\n",
    "def linearTransform(M, c, l):\n",
    "    return M.dot(getBitVector(c, l)).astype('int64')\n",
    "\n",
    "def linearTransformClient(G, S, T, l):\n",
    "    return keySwitchMatrix(G.dot(S), T,l)\n",
    "\n",
    "def vectorize(M):\n",
    "    ans = np.zeros((len(M) * len(M[0]),1))\n",
    "    for i in range(len(M)):\n",
    "        for j in range(len(M[0])):\n",
    "            ans[i * len(M[0]) + j][0] = M[i][j]\n",
    "    return ans\n",
    "\n",
    "def decrypt(S, c,w):\n",
    "    Sc = S.dot(c)\n",
    "    return (Sc / w).astype('float').round().astype('int')\n",
    "\n",
    "def innerProdClient(T,l):\n",
    "    S = getSecretKey(T)\n",
    "    tvsts = vectorize(S.T.dot(S)).T\n",
    "    mvsts = copyRows(tvsts, len(T))\n",
    "    return keySwitchMatrix(mvsts,T,l)\n",
    "\n",
    "def copyRows(row, numrows):\n",
    "    ans = np.zeros((numrows, len(row[0])))\n",
    "    for i in range(len(ans)):\n",
    "        for j in range(len(ans[0])):\n",
    "            ans[i][j] = row[0][j]\n",
    "            \n",
    "    return ans\n",
    "\n",
    "def innerProd(c1, c2, M,l):\n",
    "    \n",
    "    cc1 = np.zeros((len(c1),1))\n",
    "    for i in range(len(c1)):\n",
    "        cc1[i][0] = c1[i]\n",
    "    \n",
    "    cc2 = np.zeros((1, len(c2)))\n",
    "    for i in range(len(c2)):\n",
    "        cc2[0][i] = c2[i]\n",
    "        \n",
    "    cc = vectorize(cc1.dot(cc2))\n",
    "    \n",
    "    bv = getBitVector((cc / w).round().astype('int64'),l)\n",
    "    \n",
    "    return M.dot(bv)\n",
    "\n",
    "def one_way_encrypt_vector(vector,scaling_factor = 1000):\n",
    "    padded_vector = np.random.rand(len(vector)+1)\n",
    "    padded_vector[0:len(vector)] = vector\n",
    "    \n",
    "    vec_len = len(padded_vector)\n",
    "    \n",
    "    M_temp = (M_keys[vec_len-2].T*padded_vector*scaling_factor / (vec_len-1)).T\n",
    "    e_vector = innerProd(c_ones[vec_len-2],c_ones[vec_len-2],M_temp,l)\n",
    "    return e_vector.astype('int')\n",
    "\n",
    "def load_linear_transformation(syn0_text,scaling_factor = 1000):\n",
    "    syn0_text *= scaling_factor\n",
    "    return linearTransformClient(syn0_text.T,getSecretKey(T),T,l)\n",
    "\n",
    "def s_decrypt(vec):\n",
    "    return decrypt(getSecretKey(T_keys[len(vec)-2]),vec,w)\n",
    "\n",
    "def add_vectors(x,y,scaling_factor = 10000):\n",
    "    return x + y\n",
    "\n",
    "def transpose(syn1):\n",
    "\n",
    "    rows = len(syn1)\n",
    "    cols = len(syn1[0]) - 1\n",
    "    \n",
    "    max_rc = max(rows,cols)\n",
    "    \n",
    "    syn1_c = list()\n",
    "    for i in range(len(syn1)):\n",
    "        tmp = np.zeros(max_rc+1)\n",
    "        tmp[:len(syn1[i])] = syn1[i]\n",
    "        syn1_c.append(tmp)\n",
    "    \n",
    "    syn1_c_transposed = list()\n",
    "    \n",
    "    for row_i in range(cols):\n",
    "        syn1t_column = innerProd(syn1_c[0],v_onehot[max_rc-1][row_i],M_onehot[max_rc-1][0],l) / scaling_factor\n",
    "        for col_i in range(rows-1):\n",
    "            syn1t_column += innerProd(syn1_c[col_i+1],v_onehot[max_rc-1][row_i],M_onehot[max_rc-1][col_i+1],l) / scaling_factor\n",
    "\n",
    "        syn1_c_transposed.append(syn1t_column[0:rows+1])\n",
    "    \n",
    "    return syn1_c_transposed\n",
    "\n",
    "def int2bin(x):\n",
    "    s = list()\n",
    "    mod = 2\n",
    "    while(x > 0):\n",
    "        s.append(int(x % 2))\n",
    "        x = int(x / 2)\n",
    "    return np.array(list(reversed(s))).astype('int64')\n",
    "\n",
    "\n",
    "def getBitVector(c,l):\n",
    "    m = len(c)\n",
    "    c_star = np.zeros(l * m,dtype='int64')\n",
    "    for i in range(m):\n",
    "        local_c = int(c[i])\n",
    "        if(local_c < 0):\n",
    "            local_c = -local_c\n",
    "        b = int2bin(local_c)\n",
    "        if(c[i] < 0):\n",
    "            b *= -1\n",
    "        if(c[i] == 0):\n",
    "            b *= 0\n",
    "#         try:\n",
    "        c_star[(i * l) + (l-len(b)): (i+1) * l] += b\n",
    "#         except:\n",
    "#             print(len(b))\n",
    "#             print(i)\n",
    "#             print(len(c_star[(i * l) + (l-len(b)): (i+1) * l]))\n",
    "    return c_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HAPPENS ON SECURE SERVER\n",
    "\n",
    "l = 100\n",
    "w = 2 ** 25\n",
    "\n",
    "aBound = 10\n",
    "tBound = 10\n",
    "eBound = 10\n",
    "\n",
    "max_dim = 16\n",
    "\n",
    "scaling_factor = 1000\n",
    "\n",
    "# keys\n",
    "T_keys = list()\n",
    "for i in range(max_dim):\n",
    "    T_keys.append(np.random.rand(i+1,1))\n",
    "\n",
    "# one way encryption transformation\n",
    "M_keys = list()\n",
    "for i in range(max_dim):\n",
    "    M_keys.append(innerProdClient(T_keys[i],l))\n",
    "\n",
    "M_onehot = list()\n",
    "for h in range(max_dim):\n",
    "    i = h+1\n",
    "    buffered_eyes = list()\n",
    "    for row in np.eye(i+1):\n",
    "        buffer = np.ones(i+1)\n",
    "        buffer[0:i+1] = row\n",
    "        buffered_eyes.append((M_keys[i-1].T * buffer).T)\n",
    "    M_onehot.append(buffered_eyes)\n",
    "    \n",
    "c_ones = list()\n",
    "for i in range(max_dim):\n",
    "    c_ones.append(encrypt(T_keys[i],np.ones(i+1), w, l).astype('int'))\n",
    "    \n",
    "v_onehot = list()\n",
    "onehot = list()\n",
    "for i in range(max_dim):\n",
    "    eyes = list()\n",
    "    eyes_txt = list()\n",
    "    for eye in np.eye(i+1):\n",
    "        eyes_txt.append(eye)\n",
    "        eyes.append(one_way_encrypt_vector(eye,scaling_factor))\n",
    "    v_onehot.append(eyes)\n",
    "    onehot.append(eyes_txt)\n",
    "\n",
    "H_sigmoid_txt = np.zeros((5,5))\n",
    "\n",
    "H_sigmoid_txt[0][0] = 0.5\n",
    "H_sigmoid_txt[0][1] = 0.25\n",
    "H_sigmoid_txt[0][2] = -1/48.0\n",
    "H_sigmoid_txt[0][3] = 1/480.0\n",
    "H_sigmoid_txt[0][4] = -17/80640.0\n",
    "\n",
    "H_sigmoid = list()\n",
    "for row in H_sigmoid_txt:\n",
    "    H_sigmoid.append(one_way_encrypt_vector(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(layer_2_c):\n",
    "    out_rows = list()\n",
    "    for position in range(len(layer_2_c)-1):\n",
    "\n",
    "        M_position = M_onehot[len(layer_2_c)-2][0]\n",
    "\n",
    "        layer_2_index_c = innerProd(layer_2_c,v_onehot[len(layer_2_c)-2][position],M_position,l) / scaling_factor\n",
    "\n",
    "        x = layer_2_index_c\n",
    "        x2 = innerProd(x,x,M_position,l) / scaling_factor\n",
    "        x3 = innerProd(x,x2,M_position,l) / scaling_factor\n",
    "        x5 = innerProd(x3,x2,M_position,l) / scaling_factor\n",
    "        x7 = innerProd(x5,x2,M_position,l) / scaling_factor\n",
    "\n",
    "        xs = copy.deepcopy(v_onehot[5][0])\n",
    "        xs[1] = x[0]\n",
    "        xs[2] = x2[0]\n",
    "        xs[3] = x3[0]\n",
    "        xs[4] = x5[0]\n",
    "        xs[5] = x7[0]\n",
    "\n",
    "        out = mat_mul_forward(xs,H_sigmoid[0:1],scaling_factor)\n",
    "        out_rows.append(out)\n",
    "    return transpose(out_rows)[0]\n",
    "\n",
    "def load_linear_transformation(syn0_text,scaling_factor = 1000):\n",
    "    syn0_text *= scaling_factor\n",
    "    return linearTransformClient(syn0_text.T,getSecretKey(T_keys[len(syn0_text)-1]),T_keys[len(syn0_text)-1],l)\n",
    "\n",
    "def outer_product(x,y):\n",
    "    flip = False\n",
    "    if(len(x) < len(y)):\n",
    "        flip = True\n",
    "        tmp = x\n",
    "        x = y\n",
    "        y = tmp\n",
    "        \n",
    "    y_matrix = list()\n",
    "\n",
    "    for i in range(len(x)-1):\n",
    "        y_matrix.append(y)\n",
    "\n",
    "    y_matrix_transpose = transpose(y_matrix)\n",
    "\n",
    "    outer_result = list()\n",
    "    for i in range(len(x)-1):\n",
    "        outer_result.append(mat_mul_forward(x * onehot[len(x)-1][i],y_matrix_transpose,scaling_factor))\n",
    "    \n",
    "    if(flip):\n",
    "        return transpose(outer_result)\n",
    "    \n",
    "    return outer_result\n",
    "\n",
    "def mat_mul_forward(layer_1,syn1,scaling_factor):\n",
    "    \n",
    "    input_dim = len(layer_1)\n",
    "    output_dim = len(syn1)\n",
    "\n",
    "    buff = np.zeros(max(output_dim+1,input_dim+1))\n",
    "    buff[0:len(layer_1)] = layer_1\n",
    "    layer_1_c = buff\n",
    "    \n",
    "    syn1_c = list()\n",
    "    for i in range(len(syn1)):\n",
    "        buff = np.zeros(max(output_dim+1,input_dim+1))\n",
    "        buff[0:len(syn1[i])] = syn1[i]\n",
    "        syn1_c.append(buff)\n",
    "    \n",
    "    layer_2 = innerProd(syn1_c[0],layer_1_c,M_onehot[len(layer_1_c) - 2][0],l) / float(scaling_factor)\n",
    "    for i in range(len(syn1)-1):\n",
    "        layer_2 += innerProd(syn1_c[i+1],layer_1_c,M_onehot[len(layer_1_c) - 2][i+1],l) / float(scaling_factor)\n",
    "    return layer_2[0:output_dim+1]\n",
    "\n",
    "def elementwise_vector_mult(x,y,scaling_factor):\n",
    "    \n",
    "    y =[y]\n",
    "    \n",
    "    one_minus_layer_1 = transpose(y)\n",
    "\n",
    "    outer_result = list()\n",
    "    for i in range(len(x)-1):\n",
    "        outer_result.append(mat_mul_forward(x * onehot[len(x)-1][i],y,scaling_factor))\n",
    "        \n",
    "    return transpose(outer_result)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,min_count = 10,polarity_cutoff = 0.1,hidden_nodes = 8, learning_rate = 0.1):\n",
    "       \n",
    "        np.random.seed(1234)\n",
    "    \n",
    "        self.pre_process_data(reviews, polarity_cutoff, min_count)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self,reviews, polarity_cutoff,min_count):\n",
    "        \n",
    "        print(\"Pre-processing data...\")\n",
    "        \n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        print(\"Initializing Weights...\")\n",
    "        self.weights_0_1_t = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2_t = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        print(\"Encrypting Weights...\")\n",
    "        self.weights_0_1 = list()\n",
    "        for i,row in enumerate(self.weights_0_1_t):\n",
    "            sys.stdout.write(\"\\rEncrypting Weights from Layer 0 to Layer 1:\" + str(float((i+1) * 100) / len(self.weights_0_1_t))[0:4] + \"% done\")\n",
    "            self.weights_0_1.append(one_way_encrypt_vector(row,scaling_factor).astype('int64'))\n",
    "        print(\"\")\n",
    "        \n",
    "        self.weights_1_2 = list()\n",
    "        for i,row in enumerate(self.weights_1_2_t):\n",
    "            sys.stdout.write(\"\\rEncrypting Weights from Layer 1 to Layer 2:\" + str(float((i+1) * 100) / len(self.weights_1_2_t))[0:4] + \"% done\")\n",
    "            self.weights_1_2.append(one_way_encrypt_vector(row,scaling_factor).astype('int64'))           \n",
    "        self.weights_1_2 = transpose(self.weights_1_2)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            self.layer_0[0][self.word2index[word]] = 1\n",
    "\n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "\n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "\n",
    "        layer_1 = np.zeros_like(self.weights_0_1[0])\n",
    "\n",
    "        start = time.time()\n",
    "        correct_so_far = 0\n",
    "        total_pred = 0.5\n",
    "        for i in range(len(training_reviews_raw)):\n",
    "            review_indices = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "\n",
    "            layer_1 *= 0\n",
    "            for index in review_indices:\n",
    "                layer_1 += self.weights_0_1[index]\n",
    "            layer_1 = layer_1 / float(len(review_indices))\n",
    "            layer_1 = layer_1.astype('int64') # round to nearest integer\n",
    "\n",
    "            layer_2 = sigmoid(innerProd(layer_1,self.weights_1_2[0],M_onehot[len(layer_1) - 2][1],l) / float(scaling_factor))[0:2]\n",
    "\n",
    "            if(label == 'POSITIVE'):\n",
    "                layer_2_delta = layer_2 - (c_ones[len(layer_2) - 2] * scaling_factor)\n",
    "            else:\n",
    "                layer_2_delta = layer_2\n",
    "\n",
    "            weights_1_2_trans = transpose(self.weights_1_2)\n",
    "            layer_1_delta = mat_mul_forward(layer_2_delta,weights_1_2_trans,scaling_factor).astype('int64')\n",
    "\n",
    "            self.weights_1_2 -= np.array(outer_product(layer_2_delta,layer_1))  * self.learning_rate\n",
    "\n",
    "            for index in review_indices:\n",
    "                self.weights_0_1[index] -= (layer_1_delta * self.learning_rate).astype('int64')\n",
    "\n",
    "            # we're going to decrypt on the fly so we can watch what's happening\n",
    "            total_pred += (s_decrypt(layer_2)[0] / scaling_factor)\n",
    "            if((s_decrypt(layer_2)[0] / scaling_factor) >= (total_pred / float(i+2)) and label == 'POSITIVE'):\n",
    "                correct_so_far += 1\n",
    "            if((s_decrypt(layer_2)[0] / scaling_factor) < (total_pred / float(i+2)) and label == 'NEGATIVE'):\n",
    "                correct_so_far += 1\n",
    "\n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "\n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews_raw)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 100 == 0):\n",
    "                print(i)\n",
    "\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "\n",
    "\n",
    "        # Hidden layer\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n",
      "Initializing Weights...\n",
      "Encrypting Weights...\n",
      "Encrypting Weights from Layer 0 to Layer 1:100.% done\n",
      "Encrypting Weights from Layer 1 to Layer 2:100.% done"
     ]
    }
   ],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.15,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_weights = (copy.deepcopy(mlp.weights_0_1),copy.deepcopy(mlp.weights_1_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%0\n",
      "Progress:0.41% Speed(reviews/sec):1.978 #Correct:66 #Trained:101 Training Accuracy:65.3%100\n",
      "Progress:0.83% Speed(reviews/sec):2.014 #Correct:131 #Trained:201 Training Accuracy:65.1%200\n",
      "Progress:1.25% Speed(reviews/sec):2.011 #Correct:203 #Trained:301 Training Accuracy:67.4%300\n",
      "Progress:1.66% Speed(reviews/sec):2.003 #Correct:276 #Trained:401 Training Accuracy:68.8%400\n",
      "Progress:2.08% Speed(reviews/sec):2.007 #Correct:348 #Trained:501 Training Accuracy:69.4%500\n",
      "Progress:2.5% Speed(reviews/sec):2.015 #Correct:420 #Trained:601 Training Accuracy:69.8%600\n",
      "Progress:2.91% Speed(reviews/sec):1.974 #Correct:497 #Trained:701 Training Accuracy:70.8%700\n",
      "Progress:3.33% Speed(reviews/sec):1.973 #Correct:581 #Trained:801 Training Accuracy:72.5%800\n",
      "Progress:3.75% Speed(reviews/sec):1.976 #Correct:666 #Trained:901 Training Accuracy:73.9%900\n",
      "Progress:4.16% Speed(reviews/sec):1.983 #Correct:751 #Trained:1001 Training Accuracy:75.0%1000\n",
      "Progress:4.58% Speed(reviews/sec):1.909 #Correct:835 #Trained:1101 Training Accuracy:75.8%1100\n",
      "Progress:5.0% Speed(reviews/sec):1.905 #Correct:913 #Trained:1201 Training Accuracy:76.0%1200\n",
      "Progress:5.41% Speed(reviews/sec):1.887 #Correct:987 #Trained:1301 Training Accuracy:75.8%1300\n",
      "Progress:5.83% Speed(reviews/sec):1.891 #Correct:1069 #Trained:1401 Training Accuracy:76.3%1400\n",
      "Progress:6.25% Speed(reviews/sec):1.888 #Correct:1146 #Trained:1501 Training Accuracy:76.3%1500\n",
      "Progress:6.66% Speed(reviews/sec):1.881 #Correct:1224 #Trained:1601 Training Accuracy:76.4%1600\n",
      "Progress:7.08% Speed(reviews/sec):1.829 #Correct:1287 #Trained:1701 Training Accuracy:75.6%1700\n",
      "Progress:7.5% Speed(reviews/sec):1.831 #Correct:1361 #Trained:1801 Training Accuracy:75.5%1800\n",
      "Progress:7.91% Speed(reviews/sec):1.839 #Correct:1437 #Trained:1901 Training Accuracy:75.5%1900\n",
      "Progress:8.33% Speed(reviews/sec):1.820 #Correct:1508 #Trained:2001 Training Accuracy:75.3%2000\n",
      "Progress:8.75% Speed(reviews/sec):1.827 #Correct:1584 #Trained:2101 Training Accuracy:75.3%2100\n",
      "Progress:8.85% Speed(reviews/sec):1.829 #Correct:1602 #Trained:2127 Training Accuracy:75.3%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-f298df4f9c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-208-b5ea40aa61ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_reviews_raw, training_labels)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mlayer_2_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mweights_1_2_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mlayer_1_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_mul_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2_delta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_1_2_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2886fea3356d>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(syn1)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msyn1t_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minnerProd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn1_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_rc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_rc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0msyn1t_column\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minnerProd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn1_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_rc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_rc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2886fea3356d>\u001b[0m in \u001b[0;36minnerProd\u001b[0;34m(c1, c2, M, l)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mbv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetBitVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_way_encrypt_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.weights_0_1 = copy.deepcopy(saved_weights[0])\n",
    "mlp.weights_1_2 = copy.deepcopy(saved_weights[1])\n",
    "mlp.learning_rate = 0.0125\n",
    "mlp.train(reviews[0:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
