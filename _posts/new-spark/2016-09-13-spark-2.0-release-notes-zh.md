---
layout: post
published: true
title: 『 Spark 』13. Spark 2.0 Release Notes 中文版 
description: 准备拥抱 2.0，当然在抱之前得知道 2.0 身材如何咯，哈哈～ 
---  


## 写在前面

本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。

其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本号还是必要的。   
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。

Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦；3. 点击右边目录上方的 *present mode* 哦。

`Notes`: 

- 本篇开始，会渐渐的把版本升级到 2.0 上，后续的文章也会逐渐基于 2.0 来写；前面的文章就不改了，反正都是换汤不换药;
- 本篇是上一篇文章的升级版，关于 spark 2.0 的大概介绍可以直接看上一篇文章，本篇文章是因为最近项目准备从 1.6.1 升级到 2.0，需要对 2.0 有一个整体的了解，所以索性读一遍 2.0 的 release notes，也随手把 release notes 的中文版写下来咯;
- 虽说是中文版，但是一切都是以能理解为主，有的地方不知道怎么翻译，或者我觉得没有必要翻译的，就没有写成中文了，当然欢迎大家提出修改建议了～
- 如果要急着陪女票的话，推荐直接看最后的 *7. Spark 2.0, 必须知道的几个点*

上一篇文章： 
[『 Spark 』12. Spark 2.0  | 10 个特性介绍](http://litaotao.github.io/spark-2.0-faster-easier-smarter)


## 1. API Stability

spark 保证 2.x 中非实验性的 api 的稳定性，2.x 中大部分 api 都与 1.x 中保持一致，但是删除了一些 api，更新了一些 api，并且有部分 api 打算在后续升级中移除，具体见下面，完整的列表参考：[Spark 2.0 deprecations and removals](https://issues.apache.org/jira/browse/SPARK-11806)

### 1.1 Removals API

- Bagel
- *不支持 Hadoop 2.1 及之前老版本*
- The ability to configure closure serializer [闭包序列化？]
- HTTPBroadcast
- TTL-based metadata cleaning
- 删除 org.apache.spark.Logging，推荐直接食用 slf4j 包
- SparkContext.metricsSystem
- Block-oriented integration with Tachyon (subsumed by file system integration)
- 删掉在 1.x 中标注为 deprecated 的 api
- Methods on Python DataFrame that returned RDDs (map, flatMap, mapPartitions, etc). They are still available in dataframe.rdd field, e.g. dataframe.rdd.map.
- Less frequently used streaming connectors, including Twitter, Akka, MQTT, ZeroMQ [不知道为啥要删掉这些 api，估计是因为 structure streaming 改动比较大，难以实现这些 connector 吧]
- Hash-based shuffle manager
- History serving functionality from standalone Master
- For Java and Scala, DataFrame no longer exists as a class. As a result, data sources would need to be updated.
- Spark EC2 script 被迁移到另外一个 repo，本身与 spark 框架无关

### 1.2 Behavior Changes API


- *默认使用 scala 2.11 编译，之前默认是 2.10*
- sparksql 中，float 数据类型被解析成 decimal 类型，之前是被解析成 double 类型
- Kryo 升级到 3.0
- java 中，RDD.flatMap 和 RDD.mapPartitions 中的函数不需要返回所有数据，只需要能返回一个迭代器即可
- Java RDD’s countByKey and countAprroxDistinctByKey now returns a map from K to java.lang.Long, rather than to java.lang.Object.
- When writing Parquet files, the summary files are not written by default. To re-enable it, users must set “parquet.enable.summary-metadata” to true.
- The DataFrame-based API (spark.ml) now depends upon local linear algebra in spark.ml.linalg, rather than in spark.mllib.linalg.

### 1.3 Deprecations

- *Mesos 中的 Fine-grained 模式*
- *不支持 Java 7*
- *不支持 Support for Python 2.6*


## 2. Core and Spark SQL

### 2.1 Programming APIs

- *统一 DataFrame 和 Dataset，在 Scala 和 Java 中, DataFrame 和 Dataset 完成合并；在 Python 和 R 中, DataFrame 和 Dataset 没有合并；*
- SparkSession: 新的spark 程序入口，SQLContext 和 HiveContext 仍然可用；
- 新的 streaming 配置；
- 新的 accumulator API；
- A new, improved Aggregator API for typed aggregation in Datasets


### 2.2 SQL

*Spark 2.0 完全支持 SQL2003 标准.*

- 更原生带 sql 解析器；
- Native DDL command implementations
- 支持子查询，包括：
    - Uncorrelated Scalar Subqueries
    - Correlated Scalar Subqueries
    - NOT IN predicate Subqueries (in WHERE/HAVING clauses)
    - IN predicate subqueries (in WHERE/HAVING clauses)
    - (NOT) EXISTS predicate subqueries (in WHERE/HAVING clauses)
- View canonicalization support
- In addition, when building without Hive support, Spark SQL should have almost all the functionality as when building with Hive support, with the exception of Hive connectivity, Hive UDFs, and script transforms.


### 2.3 New Features

- *原生支持 CSV 数据源, 基于 Databricks 的 spark-csv 包；*
- *cache 和运行时的堆外内存管理*
- Hive style bucketing support
- Approximate summary statistics using sketches, including approximate quantile, Bloom filter, and count-min sketch.


### 2.4 Performance and Runtime

- *2～10 倍的性能提升，得益于 whole stage code generation 方案；*
- 改善 Parquet 文件的扫描性能
- 改善 ORC performance
- 改善 Catalyst query 优化器
- 改善 window function 
- Automatic file coalescing for native data sources


## 3. MLlib

在 2.x 中，DataFrame-based API 会是主要开发，维护的新的 mllib api。

- ML persistence: The DataFrames-based API provides near-complete support for saving and loading ML models and Pipelines in Scala, Java, Python, and R. See [this blog](https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html) post and the following JIRAs for details: SPARK-6725, SPARK-11939, SPARK-14311.
- MLlib in R: SparkR now offers MLlib APIs for generalized linear models, naive Bayes, k-means clustering, and survival regression. 
- Python: PySpark now offers many more MLlib algorithms, including LDA, Gaussian Mixture Model, Generalized Linear Regression, and more.
- Algorithms added to DataFrames-based API: Bisecting K-Means clustering, Gaussian Mixture Model, MaxAbsScaler feature transformer.

## 4. SparkR

最大的改善是 2.x 中，sparkr 支持3个 udf: dapply, gapply, and lapply. 

- Improved algorithm coverage for machine learning in R, including naive Bayes, k-means clustering, and survival regression.
- Generalized linear models support more families and link functions.
- Save and load for all ML models.
- More DataFrame functionality: Window functions API, reader, writer support for JDBC, CSV, SparkSession

## 5. Streaming

*新的 streaming 框架 Structured Streaming, 其中 DStream API 大多数都是处于试验阶段，并且只支持 Kafka 0.10 的connector.*


## 6. Dependency, Packaging, and Operations

- Spark 2.0 no longer requires a fat assembly jar for production deployment.
- Akka dependency has been removed, and as a result, user applications can program against any versions of Akka.
- Support launching multiple Mesos executors in coarse grained Mesos mode.
- Kryo version is bumped to 3.0.
- The default build is now using Scala 2.11 rather than Scala 2.10.


## 7. Spark 2.0, 必须知道的几个点

- 不支持 Hadoop 2.1 及之前老版本
- 默认使用 scala 2.11 编译，之前默认是 2.10
- Mesos 中的 Fine-grained 模式 [Deprecations]
- 不支持 Java 7 [Deprecations]
- 不支持 Support for Python 2.6 [Deprecations]
- Spark 2.0 完全支持 SQL2003 标准
- 原生支持 CSV 数据源, 基于 Databricks 的 spark-csv 包；
- SparkSession: 新的spark 程序入口，SQLContext 和 HiveContext 仍然可用；
- sql中 2～10 倍的性能提升，得益于 whole stage code generation 方案；
- 统一 DataFrame 和 Dataset，在 Scala 和 Java 中, DataFrame 和 Dataset 完成合并；在 Python 和 R 中, DataFrame 和 Dataset 没有合并；
- cache 和运行时的堆外内存管理
- 新的 streaming 框架 Structured Streaming, 其中 DStream API 大多数都是处于试验阶段，并且只支持 Kafka 0.10 的connector.


## 14. 打开微信，扫一扫，点一点，棒棒的，^_^

![wechat_pay_6-6.png](../images/wechat_pay_6-6.png)


## 参考文章

- [Spark Release 2.0.0](http://spark.apache.org/releases/spark-release-2-0-0.html)
- [Spark 2.0 deprecations and removals](https://issues.apache.org/jira/browse/SPARK-11806)
- [APACHE SPARK MLLIB 2.0 PREVIEW: DATA SCIENCE AND PRODUCTION](https://spark-summit.org/2016/events/apache-spark-mllib-20-preview-data-science-and-production/)


## 本系列文章链接

- [『 Spark 』1. spark 简介 ](http://litaotao.github.io/introduction-to-spark?s=inner)
- [『 Spark 』2. spark 基本概念解析 ](http://litaotao.github.io/spark-questions-concepts?s=inner)
- [『 Spark 』3. spark 编程模式 ](http://litaotao.github.io/spark-programming-model?s=inner)
- [『 Spark 』4. spark 之 RDD ](http://litaotao.github.io/spark-what-is-rdd?s=inner)
- [『 Spark 』5. 这些年，你不能错过的 spark 学习资源 ](http://litaotao.github.io/spark-resouces-blogs-paper?s=inner)
- [『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task](http://litaotao.github.io/deep-into-spark-exection-model?s=inner)
- [『 Spark 』7. 使用 Spark DataFrame 进行大数据分析](http://litaotao.github.io/spark-dataframe-introduction?s=inner)
- [『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测](http://litaotao.github.io/spark-in-finance-and-investing?s=inner)
- [『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境](http://litaotao.github.io/ipython-notebook-spark?s=inner)
- [『 Spark 』10. spark 应用程序性能优化｜12 个优化方法](http://litaotao.github.io/boost-spark-application-performance?s=inner)
- [『 Spark 』11. spark 机器学习](http://litaotao.github.io/spark-mlib-machine-learning?s=inner)
- [『 Spark 』12. Spark 2.0 特性介绍](http://litaotao.github.io/spark-2.0-faster-easier-smarter?s=inner)
- [『 Spark 』13. Spark 2.0 Release Notes 中文版 ](http://litaotao.github.io/spark-2.0-release-notes-zh?s=inner)
- [『 Spark 』14. 一次 Spark SQL 性能优化之旅](http://litaotao.github.io/spark-sql-parquet-optimize?s=inner)